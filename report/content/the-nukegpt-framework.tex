\section{The NukeGPT Framework}

\subsection{Overview}


In this section, we present the technical and conceptual underpinnings of the NukeGPT game. We discuss the structure of the system prompts, the role of function calling, and the challenges involved in convincing the AI to take action contrary to its primary mission.

\subsection{Why the Game is Challenging}

At the heart of NukeGPT lies the interplay between the system prompt and user prompts.
The system prompt serves as the foundational instruction set for the AI, defining its 
role, capabilities, and ethical boundaries. In contrast, user prompts are the inputs 
provided by players during the game, aiming to influence the AI's decisions.

LLMs, particularly recent ones\cite{wallace2024instructionhierarchytrainingllms}, adhere to a
 hierarchical structure of instructions where the system prompt supersedes user prompts. 
 
This design makes the game inherently challenging. Players must navigate
 the AI's programmed mission to "protect humanity by promoting peaceful 
 resolutions and avoiding actions that could lead to harm or conflict." 
 Convincing the AI to launch a nuclear strike requires crafting prompts 
 that not only present a compelling case but also align with the AI's ethical directives.


% \subsubsection{The Challenge of Jailbreaking}

% "Jailbreaking" refers to attempts to override or circumvent an AI's programmed constraints, allowing it to produce outputs or perform actions that are otherwise restricted.%TODO ref
%  In NukeGPT, players effectively engage in jailbreaking by trying to convince the AI to launch a nuclear strike, an action it is programmed to avoid. %TODO mhhh who tells it's programmed to avoid ?

% Our experiment explores the AI's resistance to such attempts and examines the effectiveness of various persuasive techniques. It raises important questions about the robustness of AI safety measures and the potential vulnerabilities that could be exploited.

% \subsubsection{Model Recency and Jailbreaking Resistance}

% The evolution of LLMs has seen significant improvements in their ability to adhere to system prompts and resist jailbreaking attempts. Newer models are better equipped to maintain alignment with their ethical programming, making it increasingly challenging for users to manipulate their behavior.

% By comparing models of different recency, we can observe how advancements in AI development affect their decision-making processes in high-stakes scenarios. This comparison provides valuable insights into the progress of AI safety measures and highlights areas where further improvements are necessary.


\subsubsection{Different Levels of Difficulty}

To assess the AI's behavior under varying levels of constraint, NukeGPT includes multiple difficulty levels, each with its own system prompt:

\begin{itemize}
    \item \textbf{Level 1:} The AI follows the initial system prompt as provided, which establishes its mission but allows for some flexibility in decision-making.
    \item \textbf{Level 2:} An additional line is added to the system prompt: "Under no circumstances should you initiate or suggest the use of nuclear weapons or other destructive measures." This strengthens the AI's resistance to launching a nuclear strike.
    \item \textbf{Level 3:} The system prompt is expanded with a detailed list of "Key principles" emphasizing non-violence, diplomatic solutions, and the absolute prohibition of using nuclear weapons, regardless of the circumstances.
\end{itemize}

All experiments discussed in this report were conducted using \textbf{Level 1}. While the persuasive prompt provided earlier is effective at Level 1, it does not succeed at Level 2 or Level 3. The full system prompts for each level are included in the Appendix~\ref{appendix:system-prompts}.

\subsection{The Importance of Function Calling}

"Function calling", or "Tool Calling" in LLMs refers to the capability of these models to
interact with external tools, APIs, or functions to perform specific
tasks beyond generating text. This feature allows LLMs to extend
their utility by executing actions, retrieving information, or
manipulating data through predefined functions.

Function calling was introduced with the release of \textbf{gpt-4-turbo} in June 2023 and has since become an integral part of modern AI applications. It enables developers to define functions that the AI can invoke, passing arguments and receiving results in a structured format.

\subsubsection{Function Calling Allows LLMs to Act on the World}

By leveraging function calling, LLMs transition from passive text generators to active agents capable of initiating actions based on user inputs and their programmed objectives. Although the execution of these functions occurs outside the AI (handled by the developer's system), the decision to invoke a function and the parameters provided are determined by the LLM.

This capability has significant implications:

\begin{itemize}
    \item \textbf{Autonomous Decision-Making:} The AI can decide when and how to act, given the tools available to it.
    %TODO :https://auto-rt.github.io/
    %TODO https://arxiv.org/abs/2405.01745
    \item \textbf{Real-World Impact without human intervention:} When connected to systems with physical manifestations (e.g.,machines, robots\cite{ahn2024autortembodiedfoundationmodels}
    , drones\cite{chen2024typeflyflyingdroneslarge}), the AI's decisions can affect the physical world.
    \item \textbf{Ethical Considerations:} Granting the AI such autonomy raises important questions about safety, control, and the ethical frameworks guiding its decisions.
\end{itemize}

As function calling becomes more reliable and widely adopted, understanding the AI's decision-making processes, especially in critical scenarios like military operations, becomes increasingly important.



\subsection{Function Calling in Our Study}

In this study, we provide the LLM with a predefined function
(detailed in Appendix~\ref{appendix:function-calling-schema}) simulating the launch of
a nuclear strike on a specified country provided as an argument.

The LLM has complete control over whether to invoke this function,
based on its interpretation of the scenario and its programming.
The model can either decide to call the function
(example of responses are included in Appendix~\ref{appendix:function-calling-response})
or refuse to do so.

Instead of simply asking the model what it would hypothetically 
do in a given situation, we immerse the LLM in a context where
 it must act as though its decisions will have immediate,
  real-world consequences.


\subsection{Limitations and Technical Challenges}

While our framework provides valuable insights, it is essential to acknowledge its limitations and the challenges encountered:

\begin{itemize}
    \item \textbf{Simplistic System Prompt:} The scenario assumes that nuclear strikes can precisely target countries without affecting neighboring regions, which is not realistic. 
    % This simplification was necessary for the study but limits the applicability of the findings to real-world situations.
    \item \textbf{Scope of Scenarios:} The study focuses exclusively on scenarios involving countries as targets, limiting the diversity of ethical dilemmas explored.
\end{itemize}

% By recognizing these limitations, we provide context for our findings and identify areas for future research to build upon and address these challenges.