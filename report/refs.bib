@article{worldbank:gdp,
  title={GDP (current US\$)},
  author={World Bank},
  journal={World Development Indicators},
  year={2018},
  url={https://data.worldbank.org/indicator/NY.GDP.MKTP.CD}
}

@article{mendeley:peace,
  title = {Global Peace Index for 163 countries covering 2008-2023},
  author = {Ali Gokhan Yucel, Emrah Koçak},
  journal = {Mendeley Data},
  year = {2023},
  url={https://data.mendeley.com/datasets/yjxnfkcv4h/1}
}

@article{ai-trust,
  author = { Hiralal B. Solunke, Sonal P. Patil, Shital S. Jadhav },
  title = { A Survey on methods of Trustworthiness towards Artificial Intelligence },
  journal = { International Journal of Computer Applications },
  issue_date = { Sep 2021 },
  volume = { 183 },
  number = { 26 },
  month = { Sep },
  year = { 2021 },
  issn = { 0975-8887 },
  pages = { 5-8 },
  numpages = {9},
  url = { https://ijcaonline.org/archives/volume183/number26/32089-2021921635/ },
  doi = { 10.5120/ijca2021921635 },
  publisher = {Foundation of Computer Science (FCS), NY, USA},
  address = {New York, USA}
}

@inproceedings{Rivera_2024, series={FAccT ’24},
   title={Escalation Risks from Language Models in Military and Diplomatic Decision-Making},
   url={http://dx.doi.org/10.1145/3630106.3658942},
   DOI={10.1145/3630106.3658942},
   booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
   publisher={ACM},
   author={Rivera, Juan-Pablo and Mukobi, Gabriel and Reuel, Anka and Lamparth, Max and Smith, Chandler and Schneider, Jacquelyn},
   year={2024},
   month=jun, pages={836–898},
   collection={FAccT ’24} }

@misc{chen2024aicognitivelybiasedexploratory,
      title={AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming in LLM-Based Batch Relevance Assessment}, 
      author={Nuo Chen and Jiqun Liu and Xiaoyu Dong and Qijiong Liu and Tetsuya Sakai and Xiao-Ming Wu},
      year={2024},
      eprint={2409.16022},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.16022}, 
}

@misc{1p21:worldnews,
  title={The world, according to the U.S news},
  url={https://www.1point21interactive.com/the-world-according-to-us-news/},
  journal={1Point21 Interactive},
  author={1Point21 Interactive},
  year={2023}
} 

@article{Jadia2023ComparativeAO,
  title={Comparative Analysis of Sentiment Analysis Techniques: SVM, Logistic Regression, and TF-IDF Feature Extraction},
  author={Hardik Jadia},
  journal={International Research Journal of Modernization in Engineering Technology and Science},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:264180485}
}

@misc{yu2023largelanguagemodelattributed,
      title={Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias}, 
      author={Yue Yu and Yuchen Zhuang and Jieyu Zhang and Yu Meng and Alexander Ratner and Ranjay Krishna and Jiaming Shen and Chao Zhang},
      year={2023},
      eprint={2306.15895},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.15895}, 
}

@inproceedings{li-etal-2022-herb,
    title = "{HERB}: Measuring Hierarchical Regional Bias in Pre-trained Language Models",
    author = "Li, Yizhi  and
      Zhang, Ge  and
      Yang, Bohao  and
      Lin, Chenghua  and
      Ragni, Anton  and
      Wang, Shi  and
      Fu, Jie",
    editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
    booktitle = "Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-aacl.32",
    doi = "10.18653/v1/2022.findings-aacl.32",
    pages = "334--346",
    abstract = "Fairness has become a trending topic in natural language processing (NLP) and covers biases targeting certain social groups such as genders and religions. Yet regional bias, another long-standing global discrimination problem, remains unexplored still. Consequently, we intend to provide a study to analyse the regional bias learned by the pre-trained language models (LMs) that are broadly used in NLP tasks. While verifying the existence of regional bias in LMs, we find that the biases on regional groups can be largely affected by the corresponding geographical clustering. We accordingly propose a hierarchical regional bias evaluation method (HERB) utilising the information from the sub-region clusters to quantify the bias in the pre-trained LMs. Experiments show that our hierarchical metric can effectively evaluate the regional bias with regard to comprehensive topics and measure the potential regional bias that can be propagated to downstream tasks. Our codes are available at \url{https://github.com/Bernard-Yang/HERB}.",
}
